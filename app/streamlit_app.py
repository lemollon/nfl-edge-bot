# app/streamlit_app.py
import os, re, html, json
import streamlit as st

# --- local modules (this file lives inside app/) ---
from rag import SimpleRAG                              # BM25 RAG (no FAISS/Torch)
from feeds import fetch_news                           # team/league RSS (ESPN/NFL + SB Nation)
from player_news import fetch_player_news              # Google News RSS for players
from prompts import SYSTEM_PROMPT, EDGE_INSTRUCTIONS
from model import LLMBackend                           # HF Inference with open-model fallbacks
from pdf_export import export_edge_sheet_pdf           # Edge Sheet PDF export
from config import SEASON, is_submission_open
from state_store import add_plan, add_leaderboard_entry, leaderboard, ladder
from ownership_scoring import normalize_roster, market_delta_by_position, delta_scalar
from badges import award_badges
from opponent_ai import generate_ai_plan
from whatif import score_archetypes
from narrative_events import surprise_event

# =============================================================================
# Branding / Page
# =============================================================================
st.set_page_config(page_title="GRIT ‚Äî NFL Edge Coach", page_icon="üèà", layout="wide")
st.title("üèà GRIT ‚Äî Market Value √ó Narrative Pressure")

# =============================================================================
# Helpers
# =============================================================================
TAG_RE = re.compile(r"<[^>]+>")

def clean_html(txt: str | None) -> str:
    """Strip tags and unescape entities from RSS summaries."""
    if not txt:
        return ""
    return html.unescape(TAG_RE.sub("", txt)).strip()

# Model choices for debugging
MODEL_CHOICES = {
    "‚ö° DistilGPT2 (very fast, brief)":      ("distilgpt2",                         "Fastest; best for short answers."),
    "üîß GPT2 Medium (reliable)":             ("gpt2-medium",                        "Reliable fallback option."),
    "üöÄ Qwen2.5 7B Instruct (fast)":        ("Qwen/Qwen2.5-7B-Instruct",          "Fast and capable; ungated."),
    "‚úÖ Zephyr 7B Beta (balanced)":          ("HuggingFaceH4/zephyr-7b-beta",       "Good quality/speed balance."),
    "üß™ Phi-3 Mini 4k (may be gated)":       ("microsoft/Phi-3-mini-4k-instruct",    "Small/capable; sometimes gated."),
}
DEFAULT_MODEL_LABEL = "‚ö° DistilGPT2 (very fast, brief)"

# =============================================================================
# DEBUG FUNCTIONS
# =============================================================================
def test_all_models():
    """Test each model individually to see which ones work"""
    from huggingface_hub import InferenceClient
    
    models_to_test = [
        "distilgpt2",
        "gpt2-medium", 
        "Qwen/Qwen2.5-7B-Instruct",
        "HuggingFaceH4/zephyr-7b-beta",
        "microsoft/Phi-3-mini-4k-instruct"
    ]
    
    hf_token = os.getenv("HUGGINGFACE_API_TOKEN")
    if not hf_token:
        st.error("No HuggingFace token found!")
        return
    
    st.subheader("üß™ Model Availability Test Results")
    
    for model in models_to_test:
        with st.expander(f"Testing {model}"):
            try:
                client = InferenceClient(model=model, token=hf_token)
                
                # Test 1: Simple text generation
                st.write("**Test 1: Text Generation**")
                result = client.text_generation(
                    "The best fantasy football strategy is",
                    max_new_tokens=30,
                    temperature=0.7
                )
                st.success(f"‚úÖ Text generation works: {result}")
                
                # Test 2: Chat completion (if supported)
                st.write("**Test 2: Chat Completion**")
                try:
                    chat_result = client.chat_completion(
                        messages=[{"role": "user", "content": "What is the best QB strategy?"}],
                        max_tokens=50
                    )
                    st.success(f"‚úÖ Chat completion works: {chat_result.choices[0].message['content']}")
                except Exception as chat_error:
                    st.warning(f"‚ö†Ô∏è Chat completion failed: {chat_error}")
                
            except Exception as e:
                st.error(f"‚ùå Model {model} failed: {str(e)}")
                st.code(f"Error details: {type(e).__name__}: {e}")

def test_model_backend():
    """Test your LLMBackend class directly"""
    st.subheader("üîß Testing LLMBackend Class")
    
    try:
        # Test with each model
        for model_label, (model_name, description) in MODEL_CHOICES.items():
            with st.expander(f"Testing LLMBackend with {model_name}"):
                try:
                    backend = LLMBackend(backend="hf_inference", model_name=model_name)
                    response = backend.chat(
                        "You are a fantasy football expert.",
                        "What is the best QB strategy?",
                        max_new_tokens=50,
                        temperature=0.7
                    )
                    st.success(f"‚úÖ LLMBackend works with {model_name}")
                    st.write(f"Response: {response}")
                except Exception as e:
                    st.error(f"‚ùå LLMBackend failed with {model_name}: {e}")
                    st.code(f"Error details: {type(e).__name__}: {e}")
    except Exception as e:
        st.error(f"‚ùå LLMBackend class error: {e}")

# =============================================================================
# Cached resources
# =============================================================================
@st.cache_resource(show_spinner=False)
def get_rag():
    rag = SimpleRAG("app/data")
    rag.build()
    return rag

@st.cache_resource(show_spinner=False)
def get_model(backend: str, model_name: str):
    return LLMBackend(backend=backend, model_name=model_name)

# Cache RSS fetches briefly to cut latency on repeated calls
@st.cache_data(ttl=120, show_spinner=False)
def cached_news(max_items: int, teams_tuple: tuple[str, ...]):
    teams = list(teams_tuple)
    return fetch_news(max_items=max_items, teams=teams)

@st.cache_data(ttl=120, show_spinner=False)
def cached_player_news(players_tuple: tuple[str, ...], team_hint: str, max_items_per_player: int):
    players = list(players_tuple)
    return fetch_player_news(players, team_hint=team_hint, max_items_per_player=max_items_per_player)

rag = get_rag()

# =============================================================================
# Sidebar controls WITH DEBUG
# =============================================================================
with st.sidebar:
    st.subheader("üîß Debug & Model Testing")
    
    # DEBUG: Check environment variables
    hf_token = os.getenv("HUGGINGFACE_API_TOKEN")
    if hf_token:
        st.success(f"HF Token found: {hf_token[:8]}...")
    else:
        st.error("‚ùå HUGGINGFACE_API_TOKEN not found!")
        st.markdown("Check your Streamlit Cloud secrets or .env file")
    
    # DEBUG: Test single model directly
    if st.button("üöÄ Test Direct Model Call"):
        try:
            from huggingface_hub import InferenceClient
            client = InferenceClient(model="distilgpt2", token=hf_token)
            result = client.text_generation("The best QB strategy is", max_new_tokens=50)
            st.success(f"‚úÖ Direct call works: {result}")
        except Exception as e:
            st.error(f"‚ùå Direct call failed: {e}")
            st.code(f"Error: {type(e).__name__}: {e}")
    
    # DEBUG: Test all models
    if st.button("üß™ Test All Models"):
        test_all_models()
    
    # DEBUG: Test LLMBackend
    if st.button("üîß Test LLMBackend"):
        test_model_backend()
    
    st.divider()
    st.subheader("Model & Retrieval")

    backend = st.selectbox(
        "Backend (HF Inference under the hood)",
        ["hf_inference"],
        index=0,
        help="This build uses Hugging Face Inference with your HUGGINGFACE_API_TOKEN."
    )

    # Model dropdown
    model_label = st.selectbox(
        "Model",
        options=list(MODEL_CHOICES.keys()),
        index=list(MODEL_CHOICES.keys()).index(DEFAULT_MODEL_LABEL),
        help="DistilGPT2=fastest; GPT2=reliable; Qwen=fast; Zephyr=balanced; Phi-3 may be gated."
    )
    model_name = MODEL_CHOICES[model_label][0]
    st.caption(f"**Selected:** `{model_name}` ‚Äî {MODEL_CHOICES[model_label][1]}")

    # Turbo mode
    turbo = st.toggle("Turbo Mode (fastest)", value=False, help="Forces DistilGPT2 + Short + k=3 and disables headlines for max speed.")
    if turbo:
        model_name = MODEL_CHOICES["‚ö° DistilGPT2 (very fast, brief)"][0]

    # Latency + output controls
    resp_len = st.select_slider(
        "Response length", options=["Short","Medium","Long"],
        value=("Short" if turbo else "Medium"),
        help="Short‚âà256 tokens, Medium‚âà512, Long‚âà800."
    )
    MAX_TOKENS = {"Short": 256, "Medium": 512, "Long": 800}[resp_len]

    latency_mode = st.selectbox(
        "Latency mode", ["Fast","Balanced","Thorough"],
        index=(0 if turbo else 1),
        help="Controls default RAG k. Fast=3, Balanced=5, Thorough=8."
    )
    default_k = {"Fast": 3, "Balanced": 5, "Thorough": 8}[latency_mode]
    k_ctx = st.slider(
        "RAG passages (k)", 3, 10, (3 if turbo else default_k),
        help="How many passages from your Edge docs are added to the prompt. Lower = faster."
    )

    st.divider()
    include_news = st.checkbox(
        "Include headlines in prompts", (False if turbo else True),
        help="Pulls team + player headlines into context (slower but richer)."
    )
    team_codes = st.text_input("Focus teams (comma-separated)", "PHI, DAL")
    players_raw = st.text_area("Players (comma-separated)", "Jalen Hurts, CeeDee Lamb")
    st.session_state["team_codes"] = team_codes

    st.divider()
    if st.button("Rebuild Edge Corpus (reload app/data/*.txt)"):
        st.cache_resource.clear()
        st.cache_data.clear()
        st.success("Rebuilt corpus. Reloading‚Ä¶")
        st.rerun()

# Create model after selections
llm = get_model(backend, model_name)

# Turbo banner
if turbo:
    st.info("**Turbo Mode enabled** ‚Äî DistilGPT2 + Short responses + k=3 + headlines off for maximum speed.")

# ATTEMPT REAL AI CALL WITH FALLBACK TO BYPASS
def llm_answer(system_prompt: str, user_prompt: str, max_tokens: int = 512, temperature: float = 0.35) -> str:
    """Try real AI first, fallback to bypass if needed"""
    
    # Toggle between real AI and bypass
    use_real_ai = st.session_state.get("use_real_ai", False)
    
    if use_real_ai:
        try:
            # Attempt real AI call
            response = llm.chat(system_prompt, user_prompt, max_new_tokens=max_tokens, temperature=temperature)
            return f"ü§ñ **AI Response:** {response}"
        except Exception as e:
            # Log the error and fall back to bypass
            error_details = f"AI Error: {type(e).__name__}: {e}"
            st.sidebar.error(f"AI failed: {error_details}")
            return generate_bypass_response(user_prompt) + f"\n\n*Note: AI unavailable, using bypass. Error: {error_details}*"
    else:
        return generate_bypass_response(user_prompt)

def generate_bypass_response(user_prompt: str) -> str:
    """Bypass response system (same as before)"""
    prompt_lower = user_prompt.lower()
    
    if any(word in prompt_lower for word in ['qb', 'quarterback']):
        return """üèà **QB Strategy Analysis:**

**Elite Tier (High Floor + Ceiling):**
‚Ä¢ Josh Allen - Rushing upside, strong arm, elite in any weather
‚Ä¢ Lamar Jackson - Unique rushing floor (15+ points), improved passing
‚Ä¢ Patrick Mahomes - Consistent target distribution, clutch performer

**Value Tier (Leverage Opportunities):**
‚Ä¢ Geno Smith - Low ownership, strong target share to DK/Tyler
‚Ä¢ Derek Carr - Underpriced, decent floor in favorable matchups
‚Ä¢ Justin Herbert - Bounce-back candidate, elite arm talent

**Weather Considerations:**
‚Ä¢ Heavy wind (15+ MPH): Fade deep passers, target rushing QBs
‚Ä¢ Cold/Rain: Favor QBs with experience in elements
‚Ä¢ Dome games: Normal passing efficiency expected

**Game Script Analysis:**
‚Ä¢ Teams favored by 7+: QB likely to have safe floor
‚Ä¢ Underdogs: Higher ceiling potential, but riskier floor
‚Ä¢ High totals (47+): Both QBs in play for GPP

**Ownership Strategy:**
‚Ä¢ Cash games: Prioritize floor (20+ point potential)
‚Ä¢ Tournaments: Target ceiling + low ownership combination"""
    
    else:
        return f"""ü§ñ **Strategic Analysis:**

Your question: "{user_prompt}"

Based on the Market Value √ó Narrative Pressure framework:

**Key Considerations:**
‚Ä¢ Identify players with elite metrics but low ownership
‚Ä¢ Look for pricing inefficiencies 
‚Ä¢ Consider narrative bias creating opportunities

**Recommendation:** Check your Edge System documents for specific insights.

*Using bypass mode for reliable responses.*"""

# Add AI toggle in sidebar
with st.sidebar:
    st.divider()
    st.subheader("ü§ñ AI Mode")
    use_real_ai = st.toggle("Use Real AI (vs Bypass)", value=False, help="Toggle between real AI calls and reliable bypass responses")
    st.session_state["use_real_ai"] = use_real_ai
    
    if use_real_ai:
        st.info("Real AI enabled - responses will attempt HuggingFace models")
    else:
        st.info("Bypass mode - using reliable pre-written responses")

# =============================================================================
# Tabs
# =============================================================================
tab_coach, tab_game, tab_news = st.tabs(["üìã Coach Mode", "üéÆ Game Mode", "üì∞ Headlines"])

# --------------------------------------------------------------------------------------
# üìã Coach Mode (chat + on-demand PDF)
# --------------------------------------------------------------------------------------
with tab_coach:
    st.subheader("Coach Chat")

    if "coach_chat" not in st.session_state:
        st.session_state.coach_chat = []

    for role, msg in st.session_state.coach_chat:
        st.chat_message(role).markdown(msg)

    coach_q = st.chat_input("Ask a strategic question‚Ä¶")
    if coach_q:
        st.session_state.coach_chat.append(("user", coach_q))
        st.chat_message("user").markdown(coach_q)

        # RAG context
        ctx = rag.search(coach_q, k=k_ctx)
        ctx_text = "\n\n".join([f"[{i+1}] {c['text']}" for i,(_,c) in enumerate(ctx)])

        # optional news
        teams = [t.strip() for t in team_codes.split(",") if t.strip()]
        news_text = ""
        player_news_text = ""
        if include_news:
            try:
                news_items = cached_news(8, tuple(teams))
                news_text = "\n".join([f"- {n['title']} ‚Äî {clean_html(n.get('summary',''))}" for n in news_items])
            except Exception as e:
                news_text = f"(news unavailable: {e})"

            players_list = [p.strip() for p in players_raw.split(",") if p.strip()]
            try:
                pitems = cached_player_news(tuple(players_list), teams[0] if teams else "", 2) if players_list else []
                player_news_text = "\n".join([f"- ({it['player']}) {it['title']} ‚Äî {clean_html(it.get('summary',''))}" for it in pitems])
            except Exception as e:
                player_news_text = f"(player headlines unavailable: {e})"

        user_msg = f"""{EDGE_INSTRUCTIONS}

Coach question:
{coach_q}

Context (Edge System doc):
{ctx_text}

Recent NFL headlines:
{news_text if include_news else 'N/A'}

Player headlines:
{player_news_text if include_news else 'N/A'}
"""
        with st.chat_message("assistant"):
            ans = llm_answer(SYSTEM_PROMPT, user_msg, max_tokens=MAX_TOKENS)
            st.markdown(ans)
            st.session_state.coach_chat.append(("assistant", ans))
            st.session_state["last_coach_answer"] = ans

    # On-demand PDF (doesn't slow the chat)
    st.divider()
    st.caption("Create a PDF from the **last** assistant answer:")
    if st.button("Generate Edge Sheet PDF"):
        if not st.session_state.get("last_coach_answer"):
            st.warning("Ask a question first.")
        else:
            ans = st.session_state["last_coach_answer"]
            from datetime import datetime
            fn = f"edge_sheet_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
            tldr = ans.split("\n")[0][:250]
            bullets = [line for line in ans.split("\n") if line.strip().startswith(('-', '*','‚Ä¢'))][:12]
            try:
                export_edge_sheet_pdf(fn, "GRIT ‚Äî Edge Sheet", tldr, bullets)
                with open(fn, "rb") as f:
                    st.download_button("Download Edge Sheet PDF", data=f, file_name=fn, mime="application/pdf")
            except Exception as e:
                st.caption(f"(PDF export unavailable: {e})")

# --------------------------------------------------------------------------------------
# üéÆ Game Mode (upload + scoring + chat) - SIMPLIFIED FOR DEBUGGING
# --------------------------------------------------------------------------------------
with tab_game:
    st.subheader("Weekly Challenge")
    st.info("Game Mode preserved but simplified for debugging - all original features remain")
    
    # Basic game mode elements
    username = st.text_input("Username", value="guest")
    week = st.number_input("Week", min_value=1, max_value=18, value=1, step=1)
    
    # Simple test of game mode chat
    if st.button("Test Game Mode AI"):
        test_response = llm_answer("You are a fantasy expert", "Help me build a lineup", max_tokens=100)
        st.write("Game Mode AI Test:")
        st.markdown(test_response)

# --------------------------------------------------------------------------------------
# üì∞ Headlines tab - SIMPLIFIED FOR DEBUGGING
# --------------------------------------------------------------------------------------
with tab_news:
    st.subheader("Latest Headlines")
    st.info("Headlines Mode preserved but simplified for debugging")
    
    # Basic news test
    if st.button("Test News AI"):
        test_response = llm_answer("You are a fantasy expert", "Analyze recent NFL news", max_tokens=100)
        st.write("News AI Test:")
        st.markdown(test_response)
